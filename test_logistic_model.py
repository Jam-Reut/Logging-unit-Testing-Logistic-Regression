import unittest
import logging
from logistic_model import load_data, train_model, evaluate_model, get_last_timing

# ------------------------------------------------------------
# Unit-Testklasse
# ------------------------------------------------------------
class TestLogisticRegressionModel(unittest.TestCase):

    @classmethod
    def setUpClass(cls):
        print("=" * 70)
        print("=== INITIALER REFERENZLAUF (setUpClass) ===")
        print("=" * 70 + "\n")

        df = load_data("advertising.csv")
        train_model(df)
        cls.ref_time = get_last_timing("train_model")

        # âœ… Ã„nderung 1: Robustheit â€“ falls Referenzlaufzeit fehlt
        if cls.ref_time is None:
            logging.warning("âš ï¸  WARNUNG: Referenzlaufzeit konnte nicht ermittelt werden.")
            cls.ref_time = 0.0

        print("\nðŸ’¬ Hinweis:")
        print("Die folgenden LogeintrÃ¤ge zeigen die AblÃ¤ufe beider TestfÃ¤lle.")
        print("Alles vor dem Punkt ('.') gehÃ¶rt zu Testfall 1 (predict),")
        print("ab '.2025-â€¦' beginnt Testfall 2 (train_runtime).\n")

    # ------------------------------------------------
    # TESTFALL 1 â€“ VorhersageprÃ¼fung
    # ------------------------------------------------
    def test_1_predict(self):
        print("=" * 70)
        print("TESTFALL 1: predict(): Vorhersagefunktion")
        print("=" * 70 + "\n")

        df = load_data("advertising.csv")
        model, X_test, y_test = train_model(df)
        acc, metrics_text = evaluate_model(model, X_test, y_test)

        # Nur Metriken ausgeben (evaluate_model macht print intern)
        print(f"\nErgebnis: TESTFALL 1 PASSED âœ…\n")

        self.assertGreaterEqual(acc, 0.9)

    # ------------------------------------------------
    # TESTFALL 2 â€“ LaufzeitprÃ¼fung
    # ------------------------------------------------
    def test_2_train_runtime(self):
    print("=" * 70)
    print("TESTFALL 2: fit(): Laufzeit der Trainingsfunktion")
    print("=" * 70 + "\n")

    df = load_data("advertising.csv")
    train_model(df)
    runtime = get_last_timing("train_model")

    ref = self.ref_time or 0.0
    limit = ref * 1.2 if ref > 0 else float("inf")

    # ðŸ‘‰ bisherige Analyse wird ans Ende verschoben

    if runtime <= limit:
        status = "PASSED âœ…"
        analysis_text = (
            "âœ… Laufzeit liegt innerhalb der Toleranz.\n\n"
            f"Ergebnis: TESTFALL 2 {status}\n"
        )
    else:
        status = "FAILED âŒ"
        analysis_text = (
            "âŒ Laufzeit Ã¼berschreitet das Limit!\n\n"
            f"Ergebnis: TESTFALL 2 {status}\n"
        )

        self.fail(
            f"âŒ Trainingslaufzeit Ã¼berschreitet das erlaubte Limit: "
            f"Aktuell {runtime:.4f}s > {limit:.4f}s (Referenz: {ref:.4f}s). "
            f"Ãœberschreitung: +{runtime - limit:.4f}s."
        )

    # ðŸ‘‰ Ausgabe der Analyse erst NACH den LogeintrÃ¤gen
    print("\nLaufzeitanalyse:")
    print("  (Referenzzeit = aus setUpClass())")
    print(f" - Referenzlaufzeit: {ref:.4f} sec")
    print("  (Aktuelle Laufzeit = aktueller Testlauf)")
    print(f" - Aktuelle Laufzeit: {runtime:.4f} sec")
    print(f" - Erlaubtes Limit (120%): {limit:.4f} sec\n")
    print(analysis_text)


if __name__ == "__main__":
    unittest.main(argv=[""], exit=False)
